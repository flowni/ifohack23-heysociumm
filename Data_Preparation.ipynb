{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import folium\n",
    "from scipy.stats import pearsonr\n",
    "import streamlit as st\n",
    "from streamlit_folium import folium_static\n",
    "from pysal.lib import weights  \n",
    "import segregation as seg\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import shap\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read zensus data in 100m x 100m grid\n",
    "df1 = pd.read_csv('data/Zensus_Bremen_Buildings.csv', delimiter=';', index_col=0)\n",
    "df2 = pd.read_csv('data/Zensus_Bremen_Families.csv', delimiter=';', index_col=0)\n",
    "df3 = pd.read_csv('data/Zensus_Bremen_Households.csv', delimiter=';', index_col=0)\n",
    "df4 = pd.read_csv('data/Zensus_Bremen_Population.csv', delimiter=';', index_col=0)\n",
    "df5 = gpd.read_file('data/Zensus_Bremen_Grid_100m.gpkg')\n",
    "df6 = gpd.read_file('data/Neighborhoods_Bremen.gpkg')\n",
    "# Load the neighborhood polygons\n",
    "neighborhood_polygons = gpd.read_file('data/Land_Prices_Neighborhood_Bremen.gpkg')\n",
    "# Load vegetation data and distance from center\n",
    "df8 = pd.read_csv('data/bremen_district_spatial.csv', delimiter=',')\n",
    "df8 = df8.rename(columns=dict(zip(df8.columns, [\"Neighborhood ID\", \"Percentage of impervious land\", \"Percentage of water\", \"Percentage of vegetation\", \"Distance from center\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data to get one table to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge them into a single dataframe by the column Grid_Code\n",
    "merged_df = pd.merge(df1, df2, on='Grid_Code')\n",
    "merged_df = pd.merge(merged_df, df3, on='Grid_Code')\n",
    "merged_df = pd.merge(merged_df, df4, on='Grid_Code')\n",
    "merged_df = pd.merge(merged_df, df5, on='Grid_Code')\n",
    "\n",
    "zensus_data = gpd.GeoDataFrame(merged_df)\n",
    "# Extract the grid middle points to match them later to their respective neighborhoods\n",
    "zensus_data.geometry = zensus_data.geometry.centroid\n",
    "# Reproject the data to use the same coordinate system\n",
    "neighborhood_polygons = neighborhood_polygons.to_crs(zensus_data.crs)\n",
    "# Join the grid points into the neighborhoods and group them\n",
    "zensus_data_by_neighborhood = gpd.sjoin(zensus_data, neighborhood_polygons, how='left', op='within')\n",
    "# Drop any non numeric data\n",
    "zensus_data_by_neighborhood['BRW'] = pd.to_numeric(zensus_data_by_neighborhood.Land_Value, errors='coerce')\n",
    "# Drop grid cells with not existing housing values\n",
    "zensus_data_by_neighborhood = zensus_data_by_neighborhood.dropna(subset=['Land_Value'])\n",
    "# Remove geometry column and add polygon areas\n",
    "zensus_data_by_neighborhood = zensus_data_by_neighborhood.drop(columns=['geometry'])\n",
    "zensus_data_by_neighborhood = pd.merge(zensus_data_by_neighborhood, neighborhood_polygons[['Neighborhood_FID', 'geometry']], on='Neighborhood_FID', how='left')\n",
    "zensus_data_by_neighborhood_grouped = zensus_data_by_neighborhood.groupby('Neighborhood_FID')\n",
    "\n",
    "# rename columns\n",
    "column_names_map = {\"Area_Types\": \"Area Type\", \"buildings_total_units\": \"Total number of buildings\", \"Land_Value\": \"Euros per square meter\", \"Neighborhood_FID\": \"Neighborhood ID\", \"geometry\": \"geometry\", \"sk_germany\": \"Amount of German citizens\", \"sk_abroad\": \"Amount of foreign citizens\"}\n",
    "age_group_map = {\"alk_18_under\": \"Age Group under 18\", \"alk_18_29\": \"Age Group 18-29\", \"alk_30_49\": \"Age Group 30-49\", \"alk_50_64\": \"Age Group 50-64\", \"alk_65_over\": \"Age Group >65\"}\n",
    "building_age_map = {'j_before_1919': \"Houses with building year under 1919\",'j_1919_1948': \"Houses with building year 1919-1948\",'j_1949_1978': \"Houses with building year 1948-1978\",'j_1979_1995': \"Houses with building year 1979-1995\",'j_1996_2008': \"Houses with building year 1996-2008\",'j_2009_and_later': \"Houses with building year >2009\"}\n",
    "household_map = {\"hhgr_1_pers\": \"Household size 1\", \"hhgr_2_pers\": \"Household size 2\", \"hhgr_3_pers\": \"Household size 3\", \"hhgr_4_pers\": \"Household size 4\", \"hhgr_5_pers\": \"Household size 5\", \"hhgr_6_more\": \"Household size >=6\"}\n",
    "column_names_map.update(age_group_map)\n",
    "column_names_map.update(building_age_map)\n",
    "column_names_map.update(household_map)\n",
    "zensus_data_by_neighborhood_filtered = zensus_data_by_neighborhood[column_names_map.keys()]\n",
    "zensus_data_by_neighborhood_filtered = zensus_data_by_neighborhood_filtered.rename(columns=column_names_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = {column: 'sum' for column in column_names_map.values()}\n",
    "agg_funcs.update({\n",
    "    'Area Type': 'first',\n",
    "    'geometry': 'first',\n",
    "    'Euros per square meter': 'mean'\n",
    "})\n",
    "agg_funcs.pop('Neighborhood ID')\n",
    "\n",
    "zensus_data_by_neighborhood = zensus_data_by_neighborhood_filtered.groupby(\"Neighborhood ID\").agg(agg_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Neighborhood name\n",
    "df6 = df6.rename(columns={'Neighborhood_FID': 'Neighborhood ID', 'Neighborhood_Name': 'Neighborhood Name'})\n",
    "df6 = df6[['Neighborhood ID', 'Neighborhood Name']]\n",
    "zensus_data_by_neighborhood = pd.merge(zensus_data_by_neighborhood, df6, on='Neighborhood ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "zensus_data_by_neighborhood = pd.merge(zensus_data_by_neighborhood, df8, on='Neighborhood ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get percentage of German citizens\n",
    "german = zensus_data_by_neighborhood[\"Amount of German citizens\"]\n",
    "foreign = zensus_data_by_neighborhood[\"Amount of foreign citizens\"]\n",
    "zensus_data_by_neighborhood[\"Percentage of German citizens\"] = (german)/(german+foreign)\n",
    "\n",
    "# Get number of residents per building\n",
    "buildings = zensus_data_by_neighborhood[\"Total number of buildings\"]\n",
    "german = zensus_data_by_neighborhood[\"Amount of German citizens\"]\n",
    "foreign = zensus_data_by_neighborhood[\"Amount of foreign citizens\"]\n",
    "zensus_data_by_neighborhood[\"Amount of residents per building\"] = (german+foreign)/buildings\n",
    "\n",
    "# Normalize building data\n",
    "buildings = zensus_data_by_neighborhood[\"Total number of buildings\"]\n",
    "for house_age in building_age_map.values():\n",
    "    normalized = zensus_data_by_neighborhood[house_age]/buildings\n",
    "    normalized.replace(np.nan, 0)\n",
    "    zensus_data_by_neighborhood['Percentage of ' + house_age] = normalized\n",
    "zensus_data_by_neighborhood=zensus_data_by_neighborhood.drop(columns=building_age_map.values())\n",
    "\n",
    "# Normalize age groups\n",
    "citizens = zensus_data_by_neighborhood[\"Amount of German citizens\"] + zensus_data_by_neighborhood[\"Amount of foreign citizens\"]\n",
    "for age in age_group_map.values():\n",
    "    normalized = zensus_data_by_neighborhood[age]/citizens\n",
    "    normalized.replace(np.nan, 0)\n",
    "    zensus_data_by_neighborhood['Percentage of ' + age] = normalized\n",
    "zensus_data_by_neighborhood=zensus_data_by_neighborhood.drop(columns=age_group_map.values())\n",
    "\n",
    "# Normalize household groups\n",
    "households = zensus_data_by_neighborhood[household_map.values()].sum(axis=1)\n",
    "for household in household_map.values():\n",
    "    normalized = zensus_data_by_neighborhood[household]/households\n",
    "    normalized.replace(np.nan, 0)\n",
    "    zensus_data_by_neighborhood['Percentage of ' + household] = normalized\n",
    "zensus_data_by_neighborhood=zensus_data_by_neighborhood.drop(columns=household_map.values())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final data for visualization (drop multiple geometry data before)\n",
    "zensus_data_by_neighborhood = zensus_data_by_neighborhood.drop(columns=[\"Amount of German citizens\", \"Amount of foreign citizens\"])\n",
    "gpd.GeoDataFrame(zensus_data_by_neighborhood).to_file('bremen_merged_data.gpkg', driver='GPKG')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model / challenge eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Euros per square meter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ifohack_spatial_py310_small_v1/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ifohack_spatial_py310_small_v1/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ifohack_spatial_py310_small_v1/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Euros per square meter'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_data\u001b[39m.\u001b[39mreplace([np\u001b[39m.\u001b[39minf, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf], np\u001b[39m.\u001b[39mnan, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_data\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_data \u001b[39m=\u001b[39m X_data[\u001b[39m'\u001b[39;49m\u001b[39mEuros per square meter\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mEuros per square meter\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholas/Development/ifohack23/ifohack23-heysociumm/Data_Preparation.ipynb#Y101sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# prep data for eval\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ifohack_spatial_py310_small_v1/lib/python3.10/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ifohack_spatial_py310_small_v1/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Euros per square meter'"
     ]
    }
   ],
   "source": [
    "X_data = zensus_data_by_neighborhood.drop(columns=['geometry', 'Area Type', 'Neighborhood Name'])\n",
    "X_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_data.dropna()\n",
    "y_data = X_data['Euros per square meter']\n",
    "X_data.drop(columns=['Euros per square meter'], inplace=True)\n",
    "\n",
    "# prep data for eval\n",
    "X_data = X_data.rename(\n",
    "    columns={'Neighborhood ID': 'Neighborhood_FID'}\n",
    "    )\n",
    "\n",
    "challenge_eval_data = X_data\n",
    "challenge_eval_data[\"City_Name\"] = \"Bremen\"\n",
    "challenge_eval_data[\"Land_Value\"] = y_data\n",
    "\n",
    "challenge_eval_data.to_csv(\"challenge_evaluation/challenge_evaluation_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifohack_spatial_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
